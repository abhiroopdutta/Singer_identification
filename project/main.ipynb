{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the AudioSegment class for processing audio and the \n",
    "# split_on_silence function for separating out silent chunks.\n",
    "from pydub import AudioSegment \n",
    "from pydub.silence import split_on_silence\n",
    "import numpy as np, matplotlib.pyplot as plot, librosa, librosa.display, sklearn, sys\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "import os, _pickle as cPickle, warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize a chunk to a target amplitude.\n",
    "def match_target_amplitude(aChunk, target_dBFS):\n",
    "    ''' Normalize given audio chunk '''\n",
    "    change_in_dBFS = target_dBFS - aChunk.dBFS\n",
    "    return aChunk.apply_gain(change_in_dBFS)\n",
    "\n",
    "#silence removal, normalization and trimming\n",
    "def remove_silence(path):\n",
    "    # Load your audio.\n",
    "    song = AudioSegment.from_file(path)\n",
    "#     final_chunk=AudioSegment.empty()\n",
    "#     # Split track where the silence is 3 seconds or more and get chunks using \n",
    "#     # the imported function.\n",
    "#     chunks = split_on_silence (\n",
    "#         # Use the loaded audio.\n",
    "#         song, \n",
    "#         # Specify that a silent chunk must be at least 3 seconds or 3000 ms long.\n",
    "#         min_silence_len = 3000,\n",
    "#         # Consider a chunk silent if it's quieter than -16 dBFS.\n",
    "#         # (You may want to adjust this parameter.)\n",
    "#         silence_thresh = -30\n",
    "#     )\n",
    "    \n",
    "#     # Process each chunk with your parameters\n",
    "#     for i, chunk in enumerate(chunks):\n",
    "\n",
    "#         # Normalize the entire chunk.\n",
    "#         normalized_chunk = match_target_amplitude(chunk, -20.0)\n",
    "\n",
    "#         #final chunk made by joining all non silent chunks\n",
    "#         final_chunk+=normalized_chunk\n",
    "    \n",
    "#     # trimming the song, taking only 60 sec from beginning\n",
    "#     final_song = final_chunk[:60000]\n",
    "\n",
    "#     print(\"song arr length : \", final_song.__len__())\n",
    "    #convert the song into numpy array\n",
    "    song_array = song.get_array_of_samples()\n",
    "    song_array = np.array(song_array)\n",
    "    return song_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting mfccs and scaling them\n",
    "def scaled_mfccs(song_array):\n",
    "    song_array = song_array.astype(float)\n",
    "    mfccs = librosa.feature.mfcc(song_array,n_mfcc=20)\n",
    "    \n",
    "    #scaling the MFCCs such that each coefficient dimension has zero mean and unit variance\n",
    "    mfccs = sklearn.preprocessing.scale(mfccs,axis =1)\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ modeling completed for speaker: 32373500.gmm  with data point =  (480, 431)\n",
      "+ modeling completed for speaker: 32449093.gmm  with data point =  (480, 431)\n",
      "+ modeling completed for speaker: 36323632.gmm  with data point =  (480, 431)\n",
      "+ modeling completed for speaker: 497880111.gmm  with data point =  (480, 431)\n",
      "+ modeling completed for speaker: 498270772.gmm  with data point =  (480, 431)\n"
     ]
    }
   ],
   "source": [
    "#training dataset location text file\n",
    "location = \"audio_files/training/\"\n",
    "dest = \"trained_models/\"\n",
    "#24 songs of 10 sec per singer, each has mfccs of size 20x?, we stack them vertically\n",
    "features = np.empty([480, 431])\n",
    "count = 1 \n",
    "i = 0\n",
    "with open(\"train.txt\", \"r\") as training_file:\n",
    "    for path in training_file:\n",
    "        #remove leading and trailing spaces\n",
    "        path = path.strip()\n",
    "        song_array = remove_silence(location+path)\n",
    "        mfccs = scaled_mfccs(song_array)\n",
    "#         np.set_printoptions(threshold=sys.maxsize)\n",
    "        \n",
    "        if(count <= 24):\n",
    "            features[i:i+20, :] = mfccs\n",
    "            i = i+20\n",
    "        if(count == 24):  \n",
    "            gmm =  GMM(n_components=1).fit(features)\n",
    "        \n",
    "            #dump the results in pickle file\n",
    "            picklefile = path.split(\"_\")[0]+\".gmm\"\n",
    "            cPickle.dump(gmm,open(dest + picklefile,'wb'))\n",
    "            print('+ modeling completed for speaker:',picklefile,\" with data point = \",features.shape)\n",
    "            \n",
    "            count = 0\n",
    "            i = 0\n",
    "        count = count+1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tdetected as -  32373500\n",
      "\tdetected as -  32373500\n",
      "\tdetected as -  32373500\n",
      "\tdetected as -  32373500\n",
      "\tdetected as -  32373500\n",
      "\tdetected as -  32373500\n",
      "\tdetected as -  32373500\n",
      "\tdetected as -  32373500\n",
      "\tdetected as -  32373500\n",
      "\tdetected as -  32373500\n",
      "\tdetected as -  32373500\n",
      "\tdetected as -  32373500\n",
      "\tdetected as -  32373500\n",
      "\tdetected as -  32373500\n",
      "\tdetected as -  32373500\n",
      "\tdetected as -  32373500\n",
      "\tdetected as -  32373500\n",
      "\tdetected as -  32373500\n",
      "\tdetected as -  32373500\n",
      "\tdetected as -  32373500\n",
      "\tdetected as -  32373500\n",
      "\tdetected as -  32373500\n",
      "\tdetected as -  32373500\n",
      "\tdetected as -  32373500\n",
      "\tdetected as -  32449093\n",
      "\tdetected as -  32449093\n",
      "\tdetected as -  32449093\n",
      "\tdetected as -  32449093\n",
      "\tdetected as -  32449093\n",
      "\tdetected as -  32449093\n",
      "\tdetected as -  32449093\n",
      "\tdetected as -  32449093\n",
      "\tdetected as -  32449093\n",
      "\tdetected as -  32449093\n",
      "\tdetected as -  32449093\n",
      "\tdetected as -  32449093\n",
      "\tdetected as -  32449093\n",
      "\tdetected as -  32449093\n",
      "\tdetected as -  32449093\n",
      "\tdetected as -  32449093\n",
      "\tdetected as -  32449093\n",
      "\tdetected as -  32449093\n",
      "\tdetected as -  32449093\n",
      "\tdetected as -  32449093\n",
      "\tdetected as -  32449093\n",
      "\tdetected as -  32449093\n",
      "\tdetected as -  32449093\n",
      "\tdetected as -  32449093\n",
      "\tdetected as -  36323632\n",
      "\tdetected as -  36323632\n",
      "\tdetected as -  36323632\n",
      "\tdetected as -  36323632\n",
      "\tdetected as -  36323632\n",
      "\tdetected as -  36323632\n",
      "\tdetected as -  36323632\n",
      "\tdetected as -  36323632\n",
      "\tdetected as -  36323632\n",
      "\tdetected as -  36323632\n",
      "\tdetected as -  36323632\n",
      "\tdetected as -  36323632\n",
      "\tdetected as -  36323632\n",
      "\tdetected as -  36323632\n",
      "\tdetected as -  36323632\n",
      "\tdetected as -  36323632\n",
      "\tdetected as -  36323632\n",
      "\tdetected as -  36323632\n",
      "\tdetected as -  36323632\n",
      "\tdetected as -  36323632\n",
      "\tdetected as -  36323632\n",
      "\tdetected as -  36323632\n",
      "\tdetected as -  36323632\n",
      "\tdetected as -  36323632\n",
      "\tdetected as -  497880111\n",
      "\tdetected as -  497880111\n",
      "\tdetected as -  497880111\n",
      "\tdetected as -  497880111\n",
      "\tdetected as -  497880111\n",
      "\tdetected as -  497880111\n",
      "\tdetected as -  497880111\n",
      "\tdetected as -  497880111\n",
      "\tdetected as -  497880111\n",
      "\tdetected as -  497880111\n",
      "\tdetected as -  497880111\n",
      "\tdetected as -  497880111\n",
      "\tdetected as -  497880111\n",
      "\tdetected as -  497880111\n",
      "\tdetected as -  497880111\n",
      "\tdetected as -  497880111\n",
      "\tdetected as -  497880111\n",
      "\tdetected as -  497880111\n",
      "\tdetected as -  497880111\n",
      "\tdetected as -  497880111\n",
      "\tdetected as -  497880111\n",
      "\tdetected as -  497880111\n",
      "\tdetected as -  497880111\n",
      "\tdetected as -  497880111\n",
      "\tdetected as -  498270772\n",
      "\tdetected as -  498270772\n",
      "\tdetected as -  498270772\n",
      "\tdetected as -  498270772\n",
      "\tdetected as -  498270772\n",
      "\tdetected as -  498270772\n",
      "\tdetected as -  498270772\n",
      "\tdetected as -  498270772\n",
      "\tdetected as -  498270772\n",
      "\tdetected as -  498270772\n",
      "\tdetected as -  498270772\n",
      "\tdetected as -  498270772\n",
      "\tdetected as -  498270772\n",
      "\tdetected as -  498270772\n",
      "\tdetected as -  498270772\n",
      "\tdetected as -  498270772\n",
      "\tdetected as -  498270772\n",
      "\tdetected as -  498270772\n",
      "\tdetected as -  498270772\n",
      "\tdetected as -  498270772\n",
      "\tdetected as -  498270772\n",
      "\tdetected as -  498270772\n",
      "\tdetected as -  498270772\n",
      "\tdetected as -  498270772\n"
     ]
    }
   ],
   "source": [
    "#path to test data \n",
    "test_location = \"audio_files/training/\"\n",
    "\n",
    "#path to trained models\n",
    "modelpath = \"trained_models/\"\n",
    "\n",
    "#get a list of path of all the GMM model files \n",
    "gmm_files = [os.path.join(modelpath,file) for file in\n",
    "              os.listdir(modelpath) if file.endswith('.gmm')]\n",
    "\n",
    "#load the models from GMM files\n",
    "#models    = [cPickle.load(open(file,'rb')) for file in gmm_files]\n",
    "\n",
    "#extract the id of the speaker corresponding to each GMM model\n",
    "speakers  = [file.split(\"/\")[-1].split(\".gmm\")[0] for file\n",
    "              in gmm_files]\n",
    "\n",
    "\n",
    "with open(\"train.txt\",\"r\") as test_paths:\n",
    "    for path in test_paths:\n",
    "        path = path.strip()\n",
    "        song_array = remove_silence(test_location+path)\n",
    "        mfccs = scaled_mfccs(song_array)\n",
    "        \n",
    "        #create an empty array to store the log-likelihood corresponding to each model\n",
    "        log_likelihood = np.zeros(len(gmm_files)) \n",
    "         \n",
    "        for i in range(len(gmm_files)):\n",
    "            file = gmm_files[i]\n",
    "            gmm = cPickle.load(open(file,'rb'))  #checking with each model one by one\n",
    "            scores = np.array(gmm.score(mfccs))\n",
    "            log_likelihood[i] = scores.sum()\n",
    "        \n",
    "        #getting the index of the model giving the maximum likelihood value\n",
    "        winner = np.argmax(log_likelihood)\n",
    "        print (\"\\tdetected as - \", speakers[winner])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
